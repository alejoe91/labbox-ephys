{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load ext autoreload\n",
    "autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "import neuropixels_data_sep_2020 as nd\n",
    "import spikeextractors as se\n",
    "import numpy as np\n",
    "from scipy.stats import wasserstein_distance\n",
    "from labbox_ephys import prepare_snippets_h5\n",
    "import kachery as ka\n",
    "import h5py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_channel_neighborhoods(recording, max_dist) -> Dict[int, List[int]]:\n",
    "    channel_ids = [int(x) for x in recording.get_channel_ids()]\n",
    "    locations_by_channel = {}\n",
    "    neighborhoods = {}\n",
    "    for channel_id in channel_ids:\n",
    "        locations_by_channel[channel_id] = np.array(recording.get_channel_property(channel_id=channel_id, property_name='location'))\n",
    "    for channel_id in channel_ids:\n",
    "        neighborhood_channel_ids = []\n",
    "        home_location = locations_by_channel[channel_id]\n",
    "        for other_channel_id in channel_ids:\n",
    "            loc = locations_by_channel[int(other_channel_id)]\n",
    "            dist = np.linalg.norm(np.array(loc) - np.array(home_location))\n",
    "            if dist < max_dist:\n",
    "                neighborhood_channel_ids.append(int(other_channel_id))\n",
    "        neighborhoods[int(channel_id)] = neighborhood_channel_ids\n",
    "    return neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unit_peak_channels_hd5(h5_snippet_uri, unit_ids):\n",
    "    h5_path = ka.load_file(h5_snippet_uri)\n",
    "    unit_maximum_channels = {}\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        for unit_id in unit_ids:\n",
    "            waveforms = np.array(f.get(f'unit_waveforms/{unit_id}/waveforms'))\n",
    "            channel_ids = np.array(f.get(f'unit_waveforms/{unit_id}/channel_ids'))\n",
    "            average_waveform = np.mean(waveforms, axis=0)\n",
    "            # NOTE CHANGE from fetch_average_waveform_plot_data -- this is peak-to-trough\n",
    "            # that function does largest-amplitude-from-0 (which may be different)\n",
    "            channel_maxima = np.max(average_waveform, axis=1) - np.min(average_waveform, axis=1)\n",
    "            index_of_max_channel = np.argmax(channel_maxima)\n",
    "            max_channel_id = channel_ids[index_of_max_channel]\n",
    "            unit_maximum_channels[unit_id] = max_channel_id\n",
    "    return unit_maximum_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distances are symmetric.\n",
    "# For any meaningful comparison to happen we have to be comparing\n",
    "# units from s1 with units from s2--so we should be able to find the\n",
    "# complete list of pairs by iterating over just one of the sortings.\n",
    "def find_local_unit_pairs(sorting1, sorting2, snippet_uri_1, snippet_uri_2, channel_neighborhoods) -> Dict[int, Set[int]]:\n",
    "    pairs: Dict[int, Set[int]] = {}\n",
    "    s1_ids = sorting1.get_unit_ids()\n",
    "    s2_ids = sorting2.get_unit_ids()\n",
    "    s1_peak_channels: Dict[int, int] = find_unit_peak_channels_hd5(snippet_uri_1, s1_ids)\n",
    "    s2_peak_channels: Dict[int, int] = find_unit_peak_channels_hd5(snippet_uri_2, s2_ids)\n",
    "    s2_units_per_peak_channel: Dict[int, Set[int]] = {}\n",
    "    for channel in s2_peak_channels.values():\n",
    "        s2_units_per_peak_channel[channel] = set([unit for unit in s2_peak_channels.keys() if s2_peak_channels[unit] == channel])\n",
    "    for s1_unit in s1_ids:\n",
    "        peak = s1_peak_channels[s1_unit]\n",
    "        pairs[s1_unit] = set()\n",
    "        for channel in channel_neighborhoods[peak]:\n",
    "            if channel not in s2_units_per_peak_channel: continue\n",
    "            pairs[s1_unit] = pairs[s1_unit] | s2_units_per_peak_channel[channel]\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_matching_events(times1, times2, delta=10):\n",
    "    # put all times in one array, first sorting1 then sorting2\n",
    "    times_concat = np.concatenate((times1, times2))\n",
    "    # create mask to identify which of the sortings each spike comes from\n",
    "    membership = np.concatenate((np.ones(times1.shape) * 1, np.ones(times2.shape) * 2))\n",
    "    # argsort: gets a list of indices that represent the sorting order of the input\n",
    "    # (e.g. if input = [5 6 2] output would be [2 0 1] as input[2] < input[0] < input[1])\n",
    "    # so indices now has the position in a fully sorted list that each element in times_concat would have\n",
    "    indices = times_concat.argsort()\n",
    "    # this sorts every spike by time, regardless of sorting source. membership still identifies source.\n",
    "    times_concat_sorted = times_concat[indices]\n",
    "    # and this gives us a mask of which sorting each spike comes from, in same order as times_concat_sorted\n",
    "    membership_sorted = membership[indices]\n",
    "\n",
    "    # difference between each element and the followingn element (pairwise subtract [a..y] - [b..z])\n",
    "    diffs = times_concat_sorted[1:] - times_concat_sorted[:-1]\n",
    "    # & in this ctxt joins queries.This is finding those indices where diffs < delta and the two units\n",
    "    # do not belong to the same sorting (since sorting[0:-1] is the source train of the left-hand\n",
    "    # spike in the difference, and sorting[1:] is the source of the right-hand spike).\n",
    "    # np.where returns a tuple of (array of indices, type) and we only want the index array, hence [0].\n",
    "    inds = np.where((diffs <= delta) & (membership_sorted[0:-1] != membership_sorted[1:]))[0]\n",
    "    if (len(inds) == 0):\n",
    "        return 0\n",
    "    inds2 = np.where(inds[:-1] + 1 != inds[1:])[0]\n",
    "    return len(inds2) + 1\n",
    "\n",
    "def count_unmatched_events(times1, times2, matched_events_count):\n",
    "    # This is \"precision\" and \"recall\" in the \"no-ground-truth\" regime.\n",
    "    # Which is essentially symmetrical: I don't know who's right.\n",
    "    # So all I can say is \"A found this many that B didn't\".\n",
    "    # It might be worthwhile to set some sort of confidence thresholds, etc. but that\n",
    "    # is not yet supported in the underlying sorters.\n",
    "    t1_unmatched = len(times1) - matched_events_count\n",
    "    t2_unmatched = len(times2) - matched_events_count\n",
    "    return (t1_unmatched, t2_unmatched)\n",
    "\n",
    "def jaccard_index(times1, times2, match_count):\n",
    "    # Jaccard index is |A n B| / |A U B|, i.e. count(matches)/count(distinct spikes)\n",
    "    # and of course |A U B| is |A| + |B| - |A n B|\n",
    "    all_spikes_count = len(times1) + len(times2) - match_count\n",
    "    return match_count / all_spikes_count\n",
    "\n",
    "def jaccard_distance(times1, times2, match_count):\n",
    "    # Convenience, if we want to compute distances\n",
    "    return 1 - jaccard_index(times1, times2, match_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "# See https://en.wikipedia.org/wiki/Earth_mover%27s_distance and https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wasserstein_distance.html\n",
    "def distance_score(times1, times2):\n",
    "    # NOTE: any sorter that returned confidence metrics could weight the observations by confidence\n",
    "    # since the scipy implementation allows weighting the values in the empirical distribution\n",
    "    return wasserstein_distance(times1, times2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pairs(pairs, sorting1, sorting2):\n",
    "    # Ultimately this will return a sparse matrix\n",
    "    # for now let's just make it work with lists-of-lists or something\n",
    "    distances = {}\n",
    "    for s1unit in pairs.keys():\n",
    "        times1 = sorting1.get_unit_spike_train(s1unit)\n",
    "        for s2unit in pairs[s1unit]:\n",
    "            times2 = sorting2.get_unit_spike_train(s2unit)\n",
    "            tag = (s1unit, s2unit)\n",
    "            matches = count_matching_events(times1, times2)\n",
    "            # not currently using this\n",
    "            # unmatched = count_unmatched_events(times1, times2, matches)\n",
    "            # distances maps the tag to a tuple of (wasserstein-distance, jaccard-distance).\n",
    "            distances[tag] = (distance_score(times1, times2), jaccard_distance(times1, times2, matches))\n",
    "    # Put into sparse-matrix format\n",
    "    return distances\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuropixels_data_sep_2020 as nd; import spikeextractors as se\n",
    "recording_id = 'cortexlab-single-phase-3 (ch 0-7, 10 sec)'\n",
    "recording = nd.load_recording(recording_id, download=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample usage\n",
    "\n",
    "import kachery_p2p as kp\n",
    "recording_uri = 'sha1dir://fb52d510d2543634e247e0d2d1d4390be9ed9e20.synth_magland/datasets_noise10_K10_C4/001_synth'\n",
    "recording1 = nd.LabboxEphysRecordingExtractor(recording_uri, download=True)\n",
    "sorting1_uri = 'sha1://3e411871054e5f3f1a4fabc61291db9d835c5201/3e411871054e5f3f1a4fabc61291db9d835c5201/firings.mda'\n",
    "sorting2_uri = 'sha1://88c6b5899d74e0a75735e3dcaead70b739c673e7/88c6b5899d74e0a75735e3dcaead70b739c673e7/firings.mda'\n",
    "\n",
    "sorting1_snippets_uri = 'sha1://727d9ef566cfa4ca66a1e5153f2bcf13d90d3977/snippets.h5?manifest=60c2432b24811eaeb595248efc06d00a2bd0c0b0'\n",
    "sorting2_snippets_uri = 'sha1://f78cffd6351d2c0fccb529dc1e997b86cec9ce63/snippets.h5?manifest=9602da6fbd2247f996553cc6f3cbff54b4313c46'\n",
    "\n",
    "kp.load_file(sorting1_uri)\n",
    "kp.load_file(sorting2_uri)\n",
    "sorting1 = nd.LabboxEphysSortingExtractor(sorting1_uri) # units 0 - 10\n",
    "sorting2 = nd.LabboxEphysSortingExtractor(sorting2_uri) # units 1 - 10\n",
    "# Can check these with sorting1.get_unit_ids() and sorting1.get_unit_spike_train(0)\n",
    "\n",
    "# Neighborhoods with large radius: everything is in everything's cluster\n",
    "big_neighborhoods = find_channel_neighborhoods(recording1, 50)\n",
    "big_pairs = find_local_unit_pairs(sorting1, sorting2, sorting1_snippets_uri, sorting2_snippets_uri, big_neighborhoods) # naturally does full outer product\n",
    "bigvals = compare_pairs(big_pairs, sorting1, sorting2)\n",
    "\n",
    "# Smaller radius: yields something like 0 -> 0, 1; 1->0, 1, 2; 2->1, 2, 3; 3->2, 3\n",
    "small_neighborhoods = find_channel_neighborhoods(recording1, 2)\n",
    "small_pairs = find_local_unit_pairs(sorting1, sorting2, sorting1_snippets_uri, sorting2_snippets_uri, small_neighborhoods) # does NOT compare everything to everything\n",
    "smallvals = compare_pairs(small_pairs, sorting1, sorting2)\n",
    "\n",
    "# Compare to self (to get a little insight into comparison statistics)\n",
    "mirror_pairs = find_local_unit_pairs(sorting1, sorting1, sorting1_snippets_uri, sorting1_snippets_uri, small_neighborhoods)\n",
    "mirrorvals = compare_pairs(mirror_pairs, sorting1, sorting1)\n",
    "\n",
    "print(f\"\"\"\n",
    "Big confusion matrix:\n",
    "{bigvals}\n",
    "\n",
    "More local confusion matrix:\n",
    "{smallvals}\n",
    "\n",
    "Self-Comparison:\n",
    "{mirrorvals}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On real-scale data\n",
    "\n",
    "import kachery_p2p as kp\n",
    "import time\n",
    "\n",
    "recording_uri = 'sha1://8b222e25bc4d9c792e4490ca322b5338e0795596/cortexlab-single-phase-3.json'\n",
    "recording1 = nd.LabboxEphysRecordingExtractor(recording_uri, download=True)\n",
    "sorting1_uri = 'sha1://b0ab8219bde481e029b69431d85e3e08bb833851/file.json' # Kilosort for this recording\n",
    "sorting2_uri = 'sha1://5c72c264f220bf36db4352b0f59380f5e7460bd8/file.json' # SpyKingCircus for this recording\n",
    "# precomputed and returned by labbox_ephys.prepare_snippets_h5 and now available in my kachery store.\n",
    "# Could also be presented by hither, but labbox caches and stores them anyway.\n",
    "sorting1_snippets_uri = 'sha1://5c66150c72758c3e16ba56f1585e260bf1a3328d/snippets.h5?manifest=41a009f1bde0c2c677cb680960f72b78e15aa514'\n",
    "sorting2_snippets_uri = 'sha1://c1281a2d4a47de8ff845974a88abbbda072d5c6e/snippets.h5?manifest=862ff23ab8ec58e85997642c3dbe32ff3a4636a0'\n",
    "kp.load_file(sorting1_uri)\n",
    "kp.load_file(sorting2_uri)\n",
    "\n",
    "sorting1 = nd.LabboxEphysSortingExtractor(sorting1_uri)\n",
    "sorting2 = nd.LabboxEphysSortingExtractor(sorting2_uri)\n",
    "# Can check these with sorting1.get_unit_ids() and sorting1.get_unit_spike_train(0)\n",
    "\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "# Neighborhoods with large radius: everything is in everything's cluster\n",
    "neighborhoods = find_channel_neighborhoods(recording1, 50)\n",
    "pairs = find_local_unit_pairs(sorting1, sorting2, sorting1_snippets_uri, sorting2_snippets_uri, neighborhoods) # naturally does full outer product\n",
    "vals = compare_pairs(pairs, sorting1, sorting2)\n",
    "elapsed = time.perf_counter() - start_time\n",
    "print(f'Computing neighborhoods, peak channels, and conf matrix for 2 sortings took {elapsed} s.')\n",
    "\n",
    "# Compare to self (to get a little insight into comparison statistics)\n",
    "mirror_pairs = find_local_unit_pairs(sorting1, sorting1, sorting1_snippets_uri, sorting1_snippets_uri, neighborhoods)\n",
    "start_time = time.perf_counter()\n",
    "mirrorvals = compare_pairs(mirror_pairs, sorting1, sorting1)\n",
    "elapsed = time.perf_counter() - start_time\n",
    "print(f'Computing reflection conf matrix alone took {elapsed} s.')\n",
    "\n",
    "\n",
    "\n",
    "#print(f\"\"\"\n",
    "#Confusion matrix:\n",
    "#{vals}\n",
    "#\n",
    "#Self-Comparison:\n",
    "#{mirrorvals}\n",
    "#\"\"\")\n",
    "\n",
    "print('Some arbitrary low-different pairs from the kilosort-spyking comparison:')\n",
    "for x in vals.keys():\n",
    "    if vals[x][0] > 100000: continue\n",
    "    print(f'{x}: {vals[x]}')\n",
    "\n",
    "i = 0\n",
    "print('Some instances where the Jaccard self-comparison is unstable:')\n",
    "for x in mirrorvals.keys():\n",
    "    if mirrorvals[x][0] > 100000: continue\n",
    "    if mirrorvals[x][1] == 0: continue\n",
    "    print(f'{x}: {mirrorvals[x]}')\n",
    "    i += 1\n",
    "    if i > 50: break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
